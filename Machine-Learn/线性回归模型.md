# 线性回归模型原理

线性回归目的其实很简单，就是找到一个直线(或一个平面或一个超维平面)h(x)，使得所有点到该直线的距离总和最短。

## 1.假设线性回归模型h(x)为:

![](https://github.com/sunnyjh/MachineLearn_Tuturial/blob/master/Machine-Learn/images/hx.jpg)

## 2.利用代价函数(或损失函数)J(θ)判断h(x)的好坏。当J(θ)值越小，点到h(x)的距离总和就越小。
![](https://github.com/sunnyjh/MachineLearn_Tuturial/blob/master/Machine-Learn/images/cost_function.jpg)
  
## 3.求解代价函数J(θ)，获得最终测线性回归模型h(x)，解决方法如下：
  
  - 最小二乘法：求误差的最小平方和，非线性最小二乘法无闭式解，一般通过迭代法求解；而线性最小二乘有闭式解，通过一系列的数学推导，可以得到一个标准方程，如下：
 
![](https://github.com/sunnyjh/MachineLearn_Tuturial/blob/master/Machine-Learn/images/least_square.png)

  随着问题的复杂度的增加时，最小二乘法实用性越来越低，这种条件下梯度下降法则非常有效。
   
  - 梯度下降法：利用沿着梯度下降最快的方向求偏导数，得到损失函数的全局最小值时的参数θ。
  
![](https://github.com/sunnyjh/MachineLearn_Tuturial/blob/master/Machine-Learn/images/gradient_descent.jpg)

  其中α为步幅，梯度下降法可以解释为：对J(θ)求关于θj(本例中只有θ1)的偏导数并乘以步幅，再用θj减去该值，得到的结果赋值给θj。此过程需要重复多次。步幅的选择会直接关系到梯度下降法的效果，如下图：
  
  ![](https://github.com/sunnyjh/MachineLearn_Tuturial/blob/master/Machine-Learn/images/gradient_descent_pic.jpg)
  
  当选取一个较小步幅的时候，将正确收敛
  当选取一个较大步幅的时候，将震荡收敛
  当选取一个过大步幅的时候，将无法收敛


