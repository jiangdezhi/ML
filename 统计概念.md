## 1.参数估计（parameter estimation）
    
      统计推断的一种。根据从总体中抽取的随机样书．来估计总体分布中未知参数的过程。从估计形式看，区分为点估计与区间估计：从构造估计量的方法讲，有矩法估计、最小二乘估计、似然估计、贝叶斯估计（这四种都属于点估计）等。
      要处理两个问题：（1）求出未知参数的估计量；（2）在一定信度（可靠程度）下指出所求的估计量的精度。信度一般用概率表示，如可信程度为95%；精度用估计量与被估参数（或待估参数）之间的接近程度或误差来度量
      
## 2.非参数估计

### 2.1 bootstrap(自助法）
    非参数统计中一种重要的估计统计量方差而进行区间估计的统计学方法，目的是用来判断统计量的精确程度，非正式的说，Bootstrap不是用来参数估计的(也就是说效果不会变好），而是用来判断原有估计量多大程度上是有效的。   
    核心思想和步骤如下：
    1）采用重抽样技术从原始样本中抽取一定数量（自己定义）的样本，此过程允许重复抽样。
    2）根据抽取的样本计算给定的统计量T
    3）重复上述步骤N次（一般大于1000次），得到N个统计量T。
    4）计算上述N个统计量T的样本方差，得到统计量的方差。
    5）用途：可以推测估计量的方差，估计量的偏差，估计量的置信区间
    
    
    R语言实现：与Monte Carlo比较
    # Monte Carlo
    t1 = NULL
    for (i in 1:10000){
    x1=rpois(30,10);y1=median(x1);t1=c(t1,y1)
    }
 
    # Bootstrap
    t2 = NULL
    x0 = rpois(30,10)
    for (i in 1:10000){
      x2=sample(x0,30,T);y2=median(x2);t2=c(t2,y2)
    }
 
    par(mfrow=c(1,2))
 
    hist(t1,xlab = "Median",main = "Monte Carlo")
    hist(t2,xlab = "Median",main = "Bootstrap")
    
    之后检验二者标准差，发现差别并不大：
    s1=sqrt(var(t1))
    s2=sqrt(var(t2))

## 3.glmnet (基于Elastic—net的广义线性模型）
   
    https://www.sohu.com/a/163683820_733585
   
    library(glmnet)
    data(BinomialExample)
    *拟合模型*
    fit<-glmnet(x, y,family = "binomial")
    *模型对象的提取和可视化*
    print(fit) #第一列Df是非零系数的个数，第三列和第二列分别是λ以及该λ对应的解释偏差百分比%dev。
    coef(fit, s=0.1) #coef来提取模型的系数：
    plot(fit, xvar = "dev", label = TRUE) #采用plot函数对拟合出的模型系数进行可视化，图形中的每一条曲线代表一个变量的系数。该图展示了系数随L1-范数变化的图，图形顶部的横轴表示当前选定的λ下非零系数的个数
    *预测：预测采用predict函数，参数newx用来设置输入数据，s用来设置λ的值*
    predict(fit, newx = x[1:10,], type = "class", s = c(0.05, 0.01))
    *交叉验证*
    glmnet提供了一系列的模型可供选择(随着λ的不同，估计出的模型也不尽相同)，而在大多数情况下我们需要从中挑选出一个最合适的来用就可以了。这时就需要通过交叉验证的方法来筛选最优的λ值了，cv.glmnet函数实现了这一功能
    cvfit <- cv.glmnet(x, y, family = "binomial", type.measure = "class")
    plot(cvfit)#该图中，红色的散点为交叉验证的散点图，横轴为logλ，纵轴为均方误差，每个点的标准偏差上界和下界也画出来了。图的顶部字数表示非零系数的个数，两条垂直的虚线即为交叉验证后选定的λ。
    cvfit$lambda.min #最优lamda,指交叉验证中使得均方误差最小的那个值λ
    cvfit$lambda.1se #lambda.1se为离最小均方误差一倍标准差的λ值  
    coef(cvfit, s="lambda.min") #coef函数来提取回归模型的系数
    predict(cvfit, newx = x[1:10,], s = "lambda.min", type = "class")
    http://www.voidcn.com/article/p-andmdgyy-bmt.html
    https://wenku.baidu.com/view/56f98b3d0912a216147929a4.html

## 4.使用sklearn进行数据预处理—归一化/标准化/正则化
    4.1 标准化（Z-Score）
    公式为：(X-mean)/std ,计算时对每个属性/每列分别进行。
    将数据按属性（按列进行）减去其均值，并处以其方差。得到的结果是，对于每个属性/每列来说所有数据都聚集在0附近，方差为1。包括两种实现方式：
    4.1.1 使用sklearn.preprocessing.scale()函数，可以直接将给定数据进行标准化
    import numpy as np  
    from sklearn.preprocessing import scale
    X = np.array([[ 1., -1.,  2.],
                  [ 2.,  0.,  0.],
                  [ 0.,  1., -1.]])
    X = scale(X)

    4.1.2 使用sklearn.preprocessing.StandardScaler类，使用该类的好处在于可以保存训练集中的参数（均值、方差）直接使用其对象转换测试集数据。
    from sklearn.preprocessing import StandardScaler
    import numpy as np
    X = np.array([[ 1., -1.,  2.],
                  [ 2.,  0.,  0.],
                  [ 0.,  1., -1.]])
    sc_X = StandardScaler()
    X = sc_X.fit_transform(X)
    
    4.2 正则化（Normalization）
    正则化的过程是将每个样本缩放到单位范数（每个样本的范数为1），如果后面要使用如二次型（点积）或者其它核方法计算两个样本之间的相似性这个方法会很有用。Normalization主要思想是对每个样本计算其p-范数，然后对该样本中每个元素除以该范数，这样处理的结果是使得每个处理后样本的p-范数（l1-norm,l2-norm）等于1。
             p-范数的计算公式：||X||p=(|x1|^p+|x2|^p+...+|xn|^p)^1/p
    该方法主要应用于文本分类和聚类中。例如，对于两个TF-IDF向量的l2-norm进行点积，就可以得到这两个向量的余弦相似性。两种实现方式如下：
    4.1.1 使用sklearn.preprocessing.normalize()函数，可以直接将给定数据进行正则化
    import numpy as np  
    from sklearn.preprocessing import normalize()
    X = np.array([[ 1., -1.,  2.],
                  [ 2.,  0.,  0.],
                  [ 0.,  1., -1.]])
    X = normalize(X)

    4.1.2 使用sklearn.preprocessing.Normalizer类实现对训练集和测试集的拟合和转换。
    from sklearn.preprocessing import Normalizer
    import numpy as np
    X = np.array([[ 1., -1.,  2.],
                  [ 2.,  0.,  0.],
                  [ 0.,  1., -1.]])
    sc_X = Normalizer()
    X = sc_X.fit_transform(X)
    
    
    https://www.cnblogs.com/chaosimple/p/4153167.html
    
